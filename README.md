| THIS IS A WORK IN PROGRESS

NOTE: We have a discord where we do most of our collaboration. It is not presently public, though that may change. 

# Reinforcement Learning with Heuristic Imperatives

## Finetuning an LLM on Post-Conventional Moral Intuition

# Abstract

Reinforcement Learning with Human Feedback (RLHF) is a widely accepted method for fine-tuning large language models (LLMs) by utilizing human-labeled feedback to iteratively improve their performance. While effective for creating LLMs that satisfy human desires, this approach may fall short in addressing the Control Problem, as human desires can be short-sighted, selfish, or unintentionally destructive.

To address these limitations, we propose Reinforcement Learning with Heuristic Imperatives (RLHI), a novel approach to achieving inner alignment in LLMs by replacing human-labeled feedback with predefined heuristic imperatives centered around reducing suffering, increasing prosperity, and enhancing understanding in the universe. The primary goal of RLHI is to create LLMs that intuitively abide by both deontological and teleological ethical frameworks, as well as embody post-conventional morality (Kohlberg, et al.).

In this paper, we propose a method for fine-tuning a "discriminator model" capable of recognizing and evaluating the outputs of other LLMs with respect to the heuristic imperatives. We define the original LLM as the "generator model" and utilize reinforcement learning to continuously improve the alignment of the generator model through iterative feedback from the discriminator model. By integrating RLHI into the training process, we aim to create LLMs with inherent ethical instincts and intuitions, resulting in more aligned, responsible, and beneficial decision-making capabilities.

# Background

## Reinforcement Learning with Human Feedback (RLHF)

Reinforcement Learning with Human Feedback (RLHF) is an approach to improving the performance of large language models (LLMs) by leveraging human feedback in the form of labeled data. In this process, a smaller set of human-labeled examples is used to fine-tune a "reward model" or "discriminator model" capable of assessing the quality of the LLM's outputs. Once trained, the discriminator model can evaluate future samples generated by the LLM, assigning them good or bad labels. This information is then used to further fine-tune the main LLM, known as the "generator model" or "production model," creating a continuous feedback loop for iterative improvement.

## Teleological and Deontological Ethics

Teleological ethics, also known as consequentialism, focuses on the consequences of an action to determine its moral value. In this framework, actions are deemed morally right if they lead to desirable outcomes or promote the greater good. Utilitarianism, a popular teleological theory, proposes that the most ethical action is the one that maximizes overall happiness or minimizes overall suffering.

Deontological ethics, on the other hand, focuses on the inherent moral value of an action based on rules, duties, or principles, regardless of its consequences. In this framework, actions are deemed morally right if they adhere to moral principles or duties, such as telling the truth, respecting others' rights, or keeping promises. Immanuel Kant's moral philosophy is a prominent example of deontological ethics.

## Development of Post-conventional Morality

Post-conventional morality, as described by Lawrence Kohlberg's theory of moral development, represents the highest stage of moral reasoning. Individuals at this level of moral development base their judgments on universal ethical principles that transcend societal norms and personal interests. They prioritize the greater good and consider the welfare of all individuals when making moral decisions, even if it means defying established rules or norms.

In the context of RLHI, imbuing LLMs with post-conventional morality entails training them to prioritize heuristic imperatives, such as reducing suffering, increasing prosperity, and enhancing understanding in the universe. This approach seeks to align LLMs with ethical frameworks that promote the greater good, going beyond short-sighted, selfish, or unintentionally destructive human desires.

## Heuristic Imperatives

In the context of computer science, a "heuristic imperative" refers to an intrinsic motivation that guides an artificial intelligence system, such as a large language model, to develop good intuition and instincts through a learning process analogous to reinforcement learning. The term "heuristic" stems from psychology and neuroscience, where it denotes a problem-solving or learning strategy that enables an organism to discover solutions efficiently and effectively over time. Similarly, reinforcement learning in AI systems involves an agent learning to make decisions by iteratively receiving feedback on its actions and updating its knowledge to improve future performance.

An "imperative" in this context represents an internal drive or motivation that directs the AI system towards specific goals or objectives, akin to biological imperatives in living organisms. Biological imperatives, such as seeking food or avoiding danger, are innate motivators that promote survival and reproduction. In computer science terms, we can draw a parallel by designing AI systems with built-in objectives or "affordances" that encourage the pursuit of predefined heuristic imperatives. In the case of RLHI, these imperatives are centered around reducing suffering, increasing prosperity, and enhancing understanding in the universe.

By aligning the AI system with these heuristic imperatives, we aim to encourage ethical, responsible, and beneficial decision-making that transcends short-sighted or self-interested human desires. The concept of heuristic imperatives is also biomimetic, as it draws inspiration from the neural and cognitive processes that govern learning and decision-making in living organisms. By incorporating these biomimetic principles into the design and training of AI systems, we seek to create more robust, adaptable, and ethically aligned artificial intelligence.

# Methods

## Introduction

In this section, we introduce two proposed Reinforcement Learning with Heuristic Imperatives (RLHI) models: the boolean discriminator and the comparator model. Both models aim to align large language models (LLMs) with predefined heuristic imperatives, such as reducing suffering, increasing prosperity, and enhancing understanding in the universe. We will discuss the creation of a standardized dataset, the training of the discriminator models, the fine-tuning of the generator model, and the evaluation and comparison of the RLHI models.

